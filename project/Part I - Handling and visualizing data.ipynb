{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I\n",
    "## Handling and visualizing data\n",
    "\n",
    "In this notebook we will download the luminosity data and we will discuss how to visualize them, how to compute secondary features from the modulation of the light sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "In this notebook we will be using:\n",
    " * `numpy` providing basic capabilities of numerical computation in Python\n",
    " * `pandas` adding to numpy a database-like access to data stored in a DataFrame data object\n",
    " * `matplotlib.pyplot` providing basic plotting capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = \"**YOUR PRESIGNED URL HERE**\"\n",
    "df = pd.read_csv(dataset_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset\n",
    "\n",
    "Let's start by looking at the dataset to grasp some general information.\n",
    "\n",
    "Visualizing the dataset in tabular form we observe that it has a first column representing the label:\n",
    " * 1: *the star is known to not have exoplanets*\n",
    " * 2: *the star is known to have exoplanets*\n",
    "followed by 3197 columns representing the luminosity as a function of time (in intervals of 30 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is rather unbalanced as we can immediately visualize with a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.LABEL.value_counts().plot.pie()\n",
    "print (df.LABEL.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the histogram of the luminosities we clearly see that the luminosities spans over a very large range of values covering multiple orders of magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = df[df.columns[1:]].values\n",
    "plt.hist(raw.flatten(), bins=100)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Star captured luminosity [a.u.]\")\n",
    "plt.ylabel(\"Number of photographies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Since we are mostly interested in effects in the frequency domain, we can clean global luminosity effects interesting the whole evolution of a star. Here we are using our intuition that suggests that the fact a star is more bright should not correlate with the probability of having exoplanets (most likely it is simply closer to Earth...).\n",
    "\n",
    "Then we remove the average luminosity of each row in our table, and divide by its standard deviation.\n",
    "In data science jargon, this is named *standardization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (raw - raw.mean(axis=1, keepdims=True))/raw.std(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing some preprocessed waveform, it is evident that some of the waveforms have luminosity structures at low frequency that can hardly correlate to exoplanets (though they may indicate binary stars or similar structures...).\n",
    "For example, look at star #123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xAxis = np.arange(data.shape[1]) * 0.5 / 24\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(xAxis, data[123], label=\"Standardized lumi\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Time [days]\")\n",
    "plt.ylabel(\"Raw luminosity [a.u.]\")\n",
    "plt.legend(title=\"Star 123\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [scipy](https://docs.scipy.org), we can easily define a digital filter to retrieve the low-frequency modulation and possibly subtract it from the signal.\n",
    "Digital filtering in scipy is implemented through the [filtfilt](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.filtfilt.html) function, combined with a function defining the filter transform. For example, *butter* defines a [Butterworth filter](https://en.wikipedia.org/wiki/Butterworth_filter) mimicking an ideal passive electronic filter.\n",
    "\n",
    "The first argument of the `butter` function is the order of the filter, the second one is the cutoff frequency.\n",
    "Not that the frequency is expressed as a function of the Nyquist frequency to be independent of the actual sampling rate. \n",
    "\n",
    "In this problem the Nyquist frequency is \n",
    "$$\n",
    "f_N = \\frac{1}{2}\\ \\frac{1}{30 \\cdot 60} = 2.8\\times 10^{-4}\\ \\mathrm{Hz} = 1\\, \\mathrm{hour}^{-1}\n",
    "$$\n",
    "\n",
    "We design a filter with a cutoff frequency of $0.5\\ \\mathrm{day}^{-1}$ which roughly correspond to average everything happening within the same day (on Earth).\n",
    "\n",
    "> **Feel free to play with the filter definition to see how the filter function changes modifying order and cutoff!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "b, a = butter(4, 0.5/24)\n",
    "filtered = filtfilt(b, a, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the effect of the filter we superpose the original and filtered version of the luminosity stream of star #123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(xAxis, data[123], label=\"Standardized lumi\")\n",
    "plt.plot(xAxis, filtered[123], label=\"Filterered lumi\", linewidth=2)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Time [days]\")\n",
    "plt.ylabel(\"Raw luminosity [a.u.]\")\n",
    "plt.legend(title=\"Star 123\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting the filtered component from the original waveform we obtain a noisy waveform that is easier to analyse programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(xAxis, data[123]-filtered[123], label=\"Fast frequency component\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Time [days]\")\n",
    "plt.ylabel(\"Raw luminosity [a.u.]\")\n",
    "plt.legend(title=\"Star 123\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting experimental biases\n",
    "\n",
    "The high frequency noise observed in the previous block may be due to experimenal differences in the sensitivity of the camera or on the exposure, rather than on real modifications of the star luminosity. If this is the case, we should see a correlation of the luminosity of most stars at the same acquisition time. \n",
    "\n",
    "To qualitatively test this hypotesis, we create figures in which the gray level corresponds to the luminosity of each star at a given instant. The human eye is rather good at noticing patterns and will tell us immediately whether this kind of correlation should be corrected for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8), dpi=100)\n",
    "plt.imshow(data, aspect='auto', cmap='gray')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Star index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the image appears completely uniform. We *know* that, even after standardization and filtering, our dataset is not constant everywhere. So what is happening? Let's draw a histogram of the standardized and filtered data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((data - filtered).flatten(), bins=100)\n",
    "plt.xlabel(\"Standardized luminosity\")\n",
    "plt.ylabel(\"Number of photographies\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there are few pixels with a very high or very low value. To convert the pixel value into a gray scale, pyplot takes the minimum and the maximum of the values to be converted and maps them into black and white, respectively. In this case, however, the vast majority of the pixels is varying on a scale which is tremendously smaller than the difference between minimum and maximum. This makes the figure unreadable.\n",
    "\n",
    "A possible solution is to discard the outlayers in order to resolve effects within the distribution core.\n",
    "\n",
    "In practice, we clip the values of the normalized and filtered data\n",
    "$$\n",
    "f(x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\ell & \\mbox{where}\\ x < \\ell\\\\\n",
    "h & \\mbox{where}\\ x > h\\\\\n",
    "x & \\mbox{elsewhere}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "where $\\ell$ and $h$ are determined inverting the relation\n",
    "$$\n",
    "\\int_{-\\infty}^{\\ell} p(L)\\mathrm dL = \\varepsilon \\qquad \\int_{h}^{+\\infty} p(L)\\mathrm dL = \\varepsilon,\n",
    "$$\n",
    "where $p(L)$ represents the distribution of luminosity.\n",
    "\n",
    "In practice, in Python it is very simple. The following function uses the functions `np.quantile` and `np.clip` to apply this transform, and then it creates the figure, taking $\\varepsilon$ as an argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_figure(data, epsilon=None):\n",
    "    plt.figure(figsize=(15,8), dpi=100)\n",
    "    if epsilon is not None:\n",
    "        low, high = np.quantile(data, [epsilon, 1-epsilon])\n",
    "        data = np.clip(data, low, high)\n",
    "    plt.imshow(data, aspect='auto', cmap='gray')\n",
    "    plt.xlabel(\"Time [$\\\\frac{1}{2}$ hour]\")\n",
    "    plt.ylabel(\"Star index\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "build_figure(data - filtered, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 - Final preprocessing ⏳\n",
    "Use the functions discussed above to replace the definition of `prep_data` below, with a version preprocessed standardizing per time instant and then per star.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the preprocessed waveforms to be classified\n",
    "\n",
    "We combine in a unique figure the luminosity stream of several stars.\n",
    "We highlight in *red* the waveforms corresponding to stars with exoplanets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,32))\n",
    "\n",
    "for star_id in range(64):\n",
    "    plt.plot(xAxis, prep_data[star_id] - 20*star_id, color='#c00' if df.LABEL.values[star_id]==2 else 'black', alpha=1)\n",
    "    plt.text(0, -20*star_id + 4, str(star_id))\n",
    "plt.xlabel(\"Time [days]\")\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 - Slicing and Visualization ⏳\n",
    "Consider stars #1 and #7 which have clear effects of exoplanets and try to zoom over the negative luminosity peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3 - Bandpass filter ⏳⏳\n",
    "Having a look to the documentation of `scipy.signal.butter` to define a bandpass filter that enhances the importance of the shadowing peaks. You can use the peaks highlighted in the previous exercise to visualize the effect of the bandpass filter.\n",
    "\n",
    "Take the minimum of the filtered waveform as a discriminant variable between stars with and without exoplanets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4 - Count threshold crossings ⏳⏳\n",
    "Define a threshold at, for example, -2, and count the number of times the waveform crosses the threshold.\n",
    "\n",
    "Crossing the threshold means sample $i$ is above threshold and sample $i+1$ is below.\n",
    "\n",
    "> **Hint.** You can use `np.count_nonzero` applied to a boolean array to count the occurences of something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4 - Count threshold crossings, with Time Below Threshold ⏳⏳⏳\n",
    "\n",
    "Extend the previous exercise by requiring that the waveform remains low by a given number of samples. \n",
    "\n",
    "> **Hint.** Consider using `np.all` for requiring that all samples in a sequence satisfy the requirement of being below the threshold. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5 - Exploit periodicity with your own C algorithm ⏳⏳⏳⏳\n",
    "\n",
    "Take a look at [this notebook](../boilerplates/CfromPython.ipynb).\n",
    "It provides an example of wrapping C code to a Python function.\n",
    "\n",
    "Starting from that example, try to implement the following algorithm.\n",
    "\n",
    "Given the brightness $b$ at time $t \\in \\{t_0, t_1, ..., t_n\\}$, $b(t)$ we define an estimator of the period as\n",
    "$$\n",
    "\\hat T = \\mathop{\\rm argmin}_{i\\, >\\, T_0}\\left[ \\mathop{\\rm min}_{j < i} \\left( \\sum_{k \\in \\{j, j+i, j + 2i...\\}} \\frac{b(t_k)}{\\left[\\mathrm{floor}\\left(\\frac{n}{i}\\right)\\right]^{\\alpha}} \\right)\\right]\n",
    "$$\n",
    "\n",
    "$T_0$ is the minimal reasonable period (works decently with $T_0 \\sim 50 \\,\\mathrm{hours}$, or $T_0 = 100$ if expressed in 30-minute time steps.\n",
    "\n",
    "$\\alpha$ is a constant that is used to enhance the importance of low frequency periodicity. Should be tuned for best discrimination power in the range $\\alpha \\in [0,  1]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.6 - Parallelize the execution on multiple threads ⏳⏳\n",
    "\n",
    "Take a look to `ThreadPool`(https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.ThreadPool) to parallelize on multiple thread the execution your C algorithm.\n",
    "\n",
    "> **Note.** You might be tempted to try using `multiprocessing.Pool` instead, DO NOT TRY! \n",
    ">\n",
    "> Forking this application with dependencies on the loaded C library results into indefinite behaviour (ahh... good old C!). \n",
    ">\n",
    "> Optimistically, it would silently crash the Python kernel behind the Jupyter notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.7 - Return also the minimum value (not only its index)\n",
    "So far we have returned a single value: the period estimator obtained with the minimization described above.\n",
    "In this exercise you are invited to modify the code to return two values.\n",
    "\n",
    "> **Hint.** A customary technique to return multiple values from a C function is to pass buffer arrays as arguments and filling them from inside the function. Since the arrays are always passed per-reference, the array allocated by the calling function will also be updated. You can use the same approach, defining numpy arrays as buffer in Python code and let them to be filled by the C function with whatever values you wish to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
